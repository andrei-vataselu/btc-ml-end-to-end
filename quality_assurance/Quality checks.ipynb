{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 2\n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 9407f28c-5f47-4d6e-b7dd-67d80ffef8c1.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2880 minutes.\nidle_timeout has been set to 2880 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 9407f28c-5f47-4d6e-b7dd-67d80ffef8c1.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 5.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 9407f28c-5f47-4d6e-b7dd-67d80ffef8c1.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 9407f28c-5f47-4d6e-b7dd-67d80ffef8c1.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 2\nSetting new number of workers to: 2\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": " \n\nfrom pyspark.sql import SparkSession, functions as F\nfrom datetime import datetime, timedelta\nimport sys\nfrom awsglue.context import GlueContext\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.job import Job\nfrom pyspark.context import SparkContext\n \ntry:\n    args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n    job_name = args['JOB_NAME']\nexcept Exception: \n    job_name = \"btc_daily_quality_manual\"\n\nglueContext = GlueContext(SparkContext.getOrCreate())\nspark = glueContext.spark_session\n\n\njob = Job(glueContext)\njob.init(job_name, {})\n\nprint(\"starting\")\n\ntoday = (datetime.utcnow() - timedelta(days=1)).strftime(\"%Y/%m/%d\")\npath = f\"s3://<your-bucket-name>/raw/stream/year={today[:4]}/month={today[5:7]}/day={today[8:10]}/\"\nprint(f\"reading from: {path}\")\n \ndf = (\n    spark.read\n         .option(\"header\", True)\n         .option(\"inferSchema\", True)\n         .csv(path)\n)\n\nprint(\"\\n schema:\")\ndf.printSchema()\ndf.show(5, truncate=False)\n \ncols = [c.lower() for c in df.columns]\nprint(f\"\\n detected columns: {cols}\")\n\nrename_map = {\n    \"epoch_ms\": \"time\",\n    \"iso_ts\": \"iso_ts\",\n    \"price_usd\": \"price_usd\",\n    \"source\": \"source\",\n    \"volume\": \"volume\",\n    \"hour\": \"hour\"   \n}\n \nselect_expr = [f\"{col} as {rename_map[col]}\" for col in cols if col in rename_map]\ndf = df.selectExpr(*select_expr)\n \ndf.printSchema()\ndf.show(5, truncate=False)\n \ntotal_rows = df.count()\nnull_prices = df.filter(F.col(\"price_usd\").isNull()).count()\nneg_prices = df.filter(F.col(\"price_usd\") < 0).count()\ndup_timestamps = df.count() - df.dropDuplicates([\"time\"]).count()\n\nprint(f\"total rows ..........: {total_rows}\")\nprint(f\"null price_usd ......: {null_prices}\")\nprint(f\"negative price_usd ..: {neg_prices}\")\nprint(f\"duplicate timestamps : {dup_timestamps}\")\n\n\nif 1000 < total_rows < 2000 and null_prices == 0 and neg_prices == 0 and dup_timestamps == 0:\n    status = \"PASSED\"\n    print(\"\\n data quality PASSED for today.\")\nelse:\n    status = \"FAILED\"\n    print(\"\\n data quality FAILED â€” investigate anomalies.\")\n\nresult_df = spark.createDataFrame(\n    [(today, total_rows, null_prices, neg_prices, dup_timestamps, status)],\n    [\"day\", \"rows\", \"null_prices\", \"neg_prices\", \"duplicate_timestamps\", \"status\"]\n)\n\nout_path = f\"s3://<your-bucket-name>/quality_reports/year={today[:4]}/month={today[5:7]}/day={today[8:10]}/\"\nresult_df.write.mode(\"overwrite\").parquet(out_path)\n\nprint(\"\\n quality check completed and report saved successfully.\")\n \nprint(\"\\n glue job completed and committed successfully.\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		}
	]
}